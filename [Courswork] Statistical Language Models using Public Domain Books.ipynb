{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Language Models using Public Domain Books ðŸ—£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Language Models (LM)\n",
    "\n",
    "In this project, we will build *[statistical language models](https://en.wikipedia.org/wiki/Language_model)* using public domain books found on [Project Gutenberg](https://www.gutenberg.org/). Language models attempt to capture the likelihood that a given sequence of words occur in a given \"language\" (the precise term is \"corpus\" or \"corpora\"). Here, \"language\" is a broad term that, in addition to the normal usage, may mean the language of a particular author or style. As with all statistical models, the true data generating process is never known and thus we cannot know the true probability that a sequence of words will occur â€“ however, we can estimate these probabilities via various methods, some of which are more reliable than others. For example, one might guess that the probability of a sentence is simply the product of the empirical probabilities (i.e., the number of times a word is observed in a dataset divided by the number of words in that dataset). This is one of the methods of estimating the probability of a sequence of words that you will implement in this project.\n",
    "\n",
    "### Tokenizing Corpora\n",
    "\n",
    "Computing the probabilities of a language model from a book requires breaking up the text of book into sequences of words. This process is called *tokenization*. In reality though, the sequences are not made up entirely of words, but rather more general terms called *tokens*. In this project tokens will include not only whole words, but also punctuation and other terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dissecting the Corpus\n",
    "\n",
    "<a name='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Corpus\n",
    "\n",
    "<a name='question1'></a>\n",
    "\n",
    "In this question, you will use `requests` to download a public domain book from [Project Gutenberg](https://www.gutenberg.org/), like [this one](http://www.gutenberg.org/files/57988/57988-0.txt), and prepare it for analysis in later questions. Create a function `get_book` that takes in the `url` of a \"Plain Text UTF-8\" book and **returns a string** containing the contents of the book. \n",
    "\n",
    "The returned string should satisfy the following conditions:\n",
    "* The contents of the book consist of **everything** between Project Gutenberg's START and END comments (but not including the START and END comments themselves).\n",
    "* The contents **do** include the title, author, and table of contents.\n",
    "* You should replace any Windows newline characters (`'\\r\\n'`) with standard newline characters (`'\\n'`).\n",
    "* You should check Project Gutenberg's [`robots.txt`](https://gutenberg.org/robots.txt) as well and implement a \"pause\" in your request in accordance with the website's policy. If the function is called twice in succession, it should not violate the `robots.txt` policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "r = requests.get('https://www.gutenberg.org/ebooks/16328.txt.utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_text = r.text\n",
    "r_text = re.split(r'\\*+.+START[A-Za-z0-9_ ]+\\*+', r_text)[1]\n",
    "r_text = re.split(r'\\*+.+END[A-Za-z0-9_ ]+\\*+', r_text)[0]\n",
    "r_text = r_text.replace('\\r\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(url):\n",
    "    \"\"\"\n",
    "    get_book that takes in the url of a 'Plain Text UTF-8' book and \n",
    "    returns a string containing the contents of the book.\n",
    "\n",
    "    The function should satisfy the following conditions:\n",
    "        - The contents of the book consist of everything between \n",
    "        Project Gutenberg's START and END comments.\n",
    "        - The contents will include title/author/table of contents.\n",
    "        - You should also transform any Windows new-lines (\\r\\n) with \n",
    "        standard new-lines (\\n).\n",
    "        - If the function is called twice in succession, it should not \n",
    "        violate the robots.txt policy.\n",
    "\n",
    "    :Example: (note '\\n' don't need to be escaped in notebooks!)\n",
    "    >>> url = 'http://www.gutenberg.org/files/57988/57988-0.txt'\n",
    "    >>> book_string = get_book(url)\n",
    "    >>> book_string[:20] == '\\\\n\\\\n\\\\n\\\\n\\\\nProduced by Chu'\n",
    "    True\n",
    "    \"\"\"\n",
    "    time.sleep(5)\n",
    "    r = requests.get(url)\n",
    "    r_text = r.text\n",
    "    r_text = re.split(r'\\*+.+START[A-Za-z0-9_ ]+\\*+', r_text)[1]\n",
    "    r_text = re.split(r'\\*+.+END[A-Za-z0-9_ ]+\\*+', r_text)[0]\n",
    "    r_text = r_text.replace('\\r\\n', '\\n')\n",
    "    return r_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Corpus\n",
    "\n",
    "<a name='question2'></a>\n",
    "\n",
    "Now, you need to **tokenize** the text of a book. To do so, create a function `tokenize` that takes in a string, `book_string`, and returns a **list of the tokens** (words, numbers, and all punctuation) satisfying the following conditions:\n",
    "* The start of every paragraph should be represented in the list with the single character `'\\x02'` (standing for START).\n",
    "* The end of every paragraph should be represented in the list with the single character `'\\x03'` (standing for STOP).\n",
    "* Tokens should include *no* whitespace.\n",
    "* Two or more newlines count as a paragraph break. Whitespace (e.g. multiple newlines) between two paragraphs of text should not appear as tokens.\n",
    "* All punctuation marks count as tokens, even if they are uncommon (e.g. `'@'`, `'+'`, and `'%'` are all valid tokens).\n",
    "\n",
    "For example, consider the following excerpt. (The first sentence is at the end of a larger paragraph, and the second sentence is at the start of a longer paragraph.)\n",
    "```\n",
    "...\n",
    "My phone's dead.\n",
    "\n",
    "I didn't get your call.\n",
    "...\n",
    "```\n",
    "should tokenize to:\n",
    "```py\n",
    "[...\n",
    "'My', 'phone', \"'\", 's', 'dead', '.', '\\x03', '\\x02', 'I', 'didn', \"'\", 't', 'get', 'your', 'call', '.'\n",
    "...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = open('data/shakespeare.txt', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', \"I'm\", 'a', 'string', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\\\n(?:\\\\n)+', 'a', t)\n",
    "re.findall(r\"[\\w']+|[.,!?;]\", \"Hello, I'm a string!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_para = re.split('\\\\n(?:\\\\n)+', t)\n",
    "split_by_para = ['\\x02 ' + split_by_para[i] + ' \\x03' for i in range(len(split_by_para))]\n",
    "t = ' '.join(split_by_para)\n",
    "t = t.replace('\\n', ' ')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(book_string):\n",
    "    \"\"\"\n",
    "    tokenize takes in book_string and outputs a list of tokens \n",
    "    satisfying the following conditions:\n",
    "        - The start of every paragraph should be represented in the \n",
    "        list with the single character \\x02 (standing for START).\n",
    "        - The end of every paragraph should be represented in the list \n",
    "        with the single character \\x03 (standing for STOP).\n",
    "        - Tokens should include no whitespace.\n",
    "        - Whitespace (e.g. multiple newlines) between two paragraphs of text \n",
    "          should be ignored, i.e. they should not appear as tokens.\n",
    "        - Two or more newlines count as a paragraph break.\n",
    "        - All punctuation marks count as tokens, even if they are \n",
    "          uncommon (e.g. `'@'`, `'+'`, and `'%'` are all valid tokens).\n",
    "\n",
    "\n",
    "    :Example:\n",
    "    >>> test_fp = os.path.join('data', 'test.txt')\n",
    "    >>> test = open(test_fp, encoding='utf-8').read()\n",
    "    >>> tokens = tokenize(test)\n",
    "    >>> tokens[0] == '\\x02'\n",
    "    True\n",
    "    >>> tokens[9] == 'dead'\n",
    "    True\n",
    "    >>> sum([x == '\\x03' for x in tokens]) == 4\n",
    "    True\n",
    "    >>> '(' in tokens\n",
    "    True\n",
    "    \"\"\"\n",
    "    t = book_string.strip()\n",
    "    split_by_para = re.split('\\\\n(?:\\\\n)+', t)\n",
    "    split_by_para = ['\\x02 ' + split_by_para[i] + ' \\x03' for i in range(len(split_by_para))]\n",
    "    t = ' '.join(split_by_para)\n",
    "    t = t.replace('\\n', ' ')\n",
    "    return re.findall(r'\\\\x02|\\\\x03|[\\w]+|[^\\w ]', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='part2'></a>\n",
    "\n",
    "## Part 2: Creating Baseline Language Models ðŸ“•\n",
    "\n",
    "Now that we're able to tokenize a corpus, it is time to start building language models (LM).\n",
    "\n",
    "In this project, we will build three different language models. They all operate under the premise of assigning probabilities to sentences. Given a sentence â€“ that is, a sequence of tokens $w = w_1\\ldots w_n$ â€“ we want to be able to compute the **probability** that sentence is used: \n",
    "$$P(w) = P(w_1,\\ldots,w_n)$$\n",
    "\n",
    "However, sentences are built from tokens, and the likelihood that a token occurs where it does depends on the tokens before it. This points to using **conditional probability** to compute $P(w)$. That is, we can write:\n",
    "\n",
    "$$\n",
    "P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})\n",
    "$$  \n",
    "This is also called the **chain rule** for probabilities.\n",
    "\n",
    "**Example:** Consider the sentence \n",
    "\n",
    "<center><code>'when I drink Coke I smile'</code></center>\n",
    "    \n",
    "The probability that it occurs, according the the chain rule, is\n",
    "\n",
    "$$\n",
    "P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I})\\cdot P(\\text{Coke | when I drink}) \\cdot P(\\text{I | when I drink Coke}) \\cdot P(\\text{smile | when I drink Coke I})\n",
    "$$\n",
    "\n",
    "That is, the probability that the sentence occurs is the product of the probability that each subsequent token follows the tokens that came before. For example, the probability $P(\\text{Coke | when I drink})$ is likely pretty high, as Coke is something that you drink. The probability $P(\\text{pizza | when I drink})$ is likely low (if not 0), because pizza is not something that you drink.\n",
    "\n",
    "You may wonder how the language model \"knows\" that Coke is something that you drink, but pizza is not. The way that the language model \"learns\" its probabilities is by **looking at examples of existing sentences**, i.e. by being **trained on a corpus**. Throughout Parts 2 and 3, you will look at **different ways of estimating these probabilities**. In each case, you will use a corpus to assign probabilities to different tokens and combinations of tokens, and use those probabilities to generate new sentences.\n",
    "\n",
    "<br>\n",
    "\n",
    "Each language model you build will be a **class** with a few methods in common:\n",
    "\n",
    "* The `__init__` constructor: when you instantiate an LM object, you will need to pass the \"training corpus\" on which your model will be trained (i.e. a list of tokens you created in Question 2 with `tokenize`). The `train` method will then use that data to create a model which is saved in the `mdl` attribute. This code is given to you.\n",
    "* The `train` method takes in a list of tokens (e.g. the output of `tokenize`) and outputs a language model. **This language model is represented as a `Series`, whose index consists of tokens and whose values are the probabilities that the tokens occur.** (In the case of the N-Gram model in Part 3/Question 5, the model will be represented as a DataFrame instead of a Series â€“ more on this later.)\n",
    "* The `probability` method takes in a sequence of tokens and returns the probability that this sequence occurs under the language model.\n",
    "* The `sample` method takes in a positive integer `M` and generates a string made up of `M` tokens using the language model. **This method generates random sentences!**\n",
    "\n",
    "Here's the general workflow:\n",
    "\n",
    "$$\\text{initialize LM with tokenized corpus} \\rightarrow \\text{train LM (i.e. assign probabilities to each token) using corpus} \\rightarrow \\text{used trained LM to compute probabilities of input sentences and generate random sentences}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Language Models\n",
    "\n",
    "<a name='question3'></a>\n",
    "\n",
    "A uniform language model is one in which any word is equally likely to appear in any position, unconditional of any other information.\n",
    "\n",
    "Let's put into context what this means by using the following example corpus:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "Given a tokenized corpus, you need to build the language model itself in `train`. As mentioned above, language models are stored as Series, where words are the index and probabilities are the values. **In a uniform language model**, the probability assigned to each token is **1 over the total number of unique tokens in the corpus**.\n",
    "\n",
    "The example corpus above has 14 **unique** tokens. This means that we'd have $P(\\text{\\x02}) = \\frac{1}{14}$, $P(\\text{when}) = \\frac{1}{14}$, and so on. Specifically, in this example, **the Series that `train` returns should contain the following values**:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{14}$ |\n",
    "| `'when'` | $\\frac{1}{14}$ |\n",
    "| `'I'` | $\\frac{1}{14}$ |\n",
    "| `'eat'` | $\\frac{1}{14}$ |\n",
    "| `'pizza'` | $\\frac{1}{14}$ |\n",
    "| `','` | $\\frac{1}{14}$ |\n",
    "| `'smile'` | $\\frac{1}{14}$ |\n",
    "| `'but'` | $\\frac{1}{14}$ |\n",
    "| `'drink'` | $\\frac{1}{14}$ |\n",
    "| `'Coke'` | $\\frac{1}{14}$ |\n",
    "| `'my'` | $\\frac{1}{14}$ |\n",
    "| `'stomach'` | $\\frac{1}{14}$ |\n",
    "| `'hurts'` | $\\frac{1}{14}$ |\n",
    "| `'\\x03'` | $\\frac{1}{14}$ |\n",
    "\n",
    "Note that:\n",
    "- **None of the probabilities we computed are conditional â€“ the uniform model does not use conditional probabilities!**\n",
    "- When looking at the Series that `train` returns (i.e. when looking at the `mdl` attribute), the `'\\x02'` and `'\\x03'` characters will show up as blank characters in the index. This is to be expected.\n",
    "- Your Series doesn't need to have the labels `'Token'` or `'Probability'` in it, like the above table does.\n",
    "\n",
    "After training the model, you need to implement two more methods, `probability` and `sample`.\n",
    "\n",
    "`probability` should take in any tuple of tokens and use the probabilities you computed in `train` (that are stored in the `mdl` attribute) to assign a probability to that sequence. For instance, suppose the input tuple is `('when', 'I', 'drink', 'Coke', 'I', 'smile')` (note that this tuple does not need to start with `'\\x02'` or end with `'\\x03'`). To compute the probability of this tuple under our language model, we would multiply the \"trained\" probabilities for each word individually. Here that would give us \n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile}) = \\left( \\frac{1}{14} \\right)^6$$\n",
    "\n",
    "Note that if the input tuple contains a token that was not in our corpus, `probability` should return 0.\n",
    "\n",
    "Finally, `sample` should take in a positive integer, `M`, and return a sentence made up of `M` randomly sampled tokens, in which the probabilities come from `mdl`. For instance, if `M=5`, then we'd return a sentence containing 5 randomly selected tokens from the table above, such that the probability that each token is selected is $\\frac{1}{14}$. The sampling is done with replacement, so we could end up with the same token multiple times. For instance, we might end up with `'but drink smile drink hurts'`. Note that this sentence doesn't make any grammatical sense (that's okay!) and that tokens are separated by spaces.\n",
    "\n",
    "The starter code contains a class `UniformLM` which represents a uniform language model. Complete the implementation of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tuple('one one two three one two four'.split())\n",
    "t = ('one', 'two')\n",
    "pd.Series(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_unique = pd.Series(tokens).unique()\n",
    "denom = len(token_unique)\n",
    "prob = [1/denom for i in range(denom)]\n",
    "mdl = pd.DataFrame({'Token':token_unique, 'Probability':prob}).set_index('Token')['Probability']\n",
    "idx = mdl.index\n",
    "p = mdl.to_list()\n",
    "np.random.choice(idx, 10, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UniformLM(object):\n",
    "    \"\"\"\n",
    "    Uniform Language Model class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        \"\"\"\n",
    "        Initializes a Uniform languange model using a\n",
    "        list of tokens. It trains the language model\n",
    "        using `train` and saves it to an attribute\n",
    "        self.mdl.\n",
    "        \"\"\"\n",
    "        self.mdl = self.train(tokens)\n",
    "        \n",
    "    def train(self, tokens):\n",
    "        \"\"\"\n",
    "        Trains a uniform language model given a list of tokens.\n",
    "        The output is a series indexed on distinct tokens, and\n",
    "        values giving the (uniform) probability of a token occuring\n",
    "        in the language.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('one one two three one two four'.split())\n",
    "        >>> unif = UniformLM(tokens)\n",
    "        >>> isinstance(unif.mdl, pd.Series)\n",
    "        True\n",
    "        >>> set(unif.mdl.index) == set('one two three four'.split())\n",
    "        True\n",
    "        >>> (unif.mdl == 0.25).all()\n",
    "        True\n",
    "        \"\"\"\n",
    "        token_unique = pd.Series(tokens).unique()\n",
    "        denom = len(token_unique)\n",
    "        prob = [1/denom for i in range(denom)]\n",
    "        series = pd.DataFrame({'Token':token_unique, 'Probability':prob}).set_index('Token')['Probability']\n",
    "        return series\n",
    "\n",
    "    def probability(self, words):\n",
    "        \"\"\"\n",
    "        probability gives the probabiliy a sequence of words\n",
    "        appears under the language model.\n",
    "        :param: words: a tuple of tokens\n",
    "        :returns: the probability `words` appears under the language\n",
    "        model.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('one one two three one two four'.split())\n",
    "        >>> unif = UniformLM(tokens)\n",
    "        >>> unif.probability(('five',))\n",
    "        0\n",
    "        >>> unif.probability(('one', 'two')) == 0.0625\n",
    "        True\n",
    "        \"\"\"\n",
    "        bool = all(word in self.mdl.index for word in words)\n",
    "        if bool:\n",
    "            return self.mdl.iloc[0]**len(words)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sample(self, M):\n",
    "        \"\"\"\n",
    "        sample selects tokens from the language model of length M, returning\n",
    "        a string of tokens.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('one one two three one two four'.split())\n",
    "        >>> unif = UniformLM(tokens)\n",
    "        >>> samp = unif.sample(1000)\n",
    "        >>> isinstance(samp, str)\n",
    "        True\n",
    "        >>> len(samp.split()) == 1000\n",
    "        True\n",
    "        >>> s = pd.Series(samp.split()).value_counts(normalize=True)\n",
    "        >>> np.isclose(s, 0.25, atol=0.05).all()\n",
    "        True\n",
    "        \"\"\"\n",
    "        idx = self.mdl.index\n",
    "        prob = self.mdl.to_list()\n",
    "        lst = np.random.choice(idx, M, p=prob)\n",
    "        return ' '.join(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Language Models\n",
    "\n",
    "<a name='question4'></a>\n",
    "\n",
    "A unigram language model is one in which the **probability assigned to a token is equal to the proportion of tokens in the corpus that are equal to said token**. That is, the probability distribution associated with a unigram language model is just the empirical distribution of tokens in the corpus. \n",
    "\n",
    "Let's understand how probabilities are assigned to tokens using our example corpus from before.\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "Here, there are 19 total tokens. 3 of them are equal to `'I'`, so $P(\\text{I}) = \\frac{3}{19}$. Here, the Series that `train` returns should contain the following values:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{19}$ |\n",
    "| `'when'` | $\\frac{2}{19}$ |\n",
    "| `'I'` | $\\frac{3}{19}$ |\n",
    "| `'eat'` | $\\frac{1}{19}$ |\n",
    "| `'pizza'` | $\\frac{1}{19}$ |\n",
    "| `','` | $\\frac{3}{19}$ |\n",
    "| `'smile'` | $\\frac{1}{19}$ |\n",
    "| `'but'` | $\\frac{1}{19}$ |\n",
    "| `'drink'` | $\\frac{1}{19}$ |\n",
    "| `'Coke'` | $\\frac{1}{19}$ |\n",
    "| `'my'` | $\\frac{1}{19}$ |\n",
    "| `'stomach'` | $\\frac{1}{19}$ |\n",
    "| `'hurts'` | $\\frac{1}{19}$ |\n",
    "| `'\\x03'` | $\\frac{1}{19}$ |\n",
    "\n",
    "As before, the `probability` method should take in a tuple and return its probability, using the probabilities stored in `mdl`. For instance, suppose the input tuple is `('when', 'I', 'drink', 'Coke', 'I', 'smile')`. Then,\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile}) = \\frac{2}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19} \\cdot \\frac{1}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19}$$\n",
    "\n",
    "The `sample` method should now account for the fact that not all tokens are equally likely to be sampled. For instance, `'I'` is much more likely to appear in a randomly generated sentence created by `sample` than `'Coke'` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UnigramLM(object):\n",
    "    \n",
    "    def __init__(self, tokens):\n",
    "        \"\"\"\n",
    "        Initializes a Unigram languange model using a\n",
    "        list of tokens. It trains the language model\n",
    "        using `train` and saves it to an attribute\n",
    "        self.mdl.\n",
    "        \"\"\"\n",
    "        self.mdl = self.train(tokens)\n",
    "    \n",
    "    def train(self, tokens):\n",
    "        \"\"\"\n",
    "        Trains a unigram language model given a list of tokens.\n",
    "        The output is a series indexed on distinct tokens, and\n",
    "        values giving the probability of a token occuring\n",
    "        in the language.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('one one two three one two four'.split())\n",
    "        >>> unig = UnigramLM(tokens)\n",
    "        >>> isinstance(unig.mdl, pd.Series)\n",
    "        True\n",
    "        >>> set(unig.mdl.index) == set('one two three four'.split())\n",
    "        True\n",
    "        >>> unig.mdl.loc['one'] == 3 / 7\n",
    "        True\n",
    "        \"\"\"\n",
    "        s = pd.Series(tokens).value_counts()\n",
    "        return s/sum(s)\n",
    "    \n",
    "    def probability(self, words):\n",
    "        \"\"\"\n",
    "        probability gives the probabiliy a sequence of words\n",
    "        appears under the language model.\n",
    "        :param: words: a tuple of tokens\n",
    "        :returns: the probability `words` appears under the language\n",
    "        model.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('one one two three one two four'.split())\n",
    "        >>> unig = UnigramLM(tokens)\n",
    "        >>> unig.probability(('five',))\n",
    "        0\n",
    "        >>> p = unig.probability(('one', 'two'))\n",
    "        >>> np.isclose(p, 0.12244897959, atol=0.0001)\n",
    "        True\n",
    "        \"\"\"\n",
    "        bool = all(word in self.mdl.index for word in words)\n",
    "        if bool:\n",
    "            s = 1\n",
    "            for word in words:\n",
    "                s = s*self.mdl.loc[word]\n",
    "            return s\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sample(self, M):\n",
    "        \"\"\"\n",
    "        sample selects tokens from the language model of length M, returning\n",
    "        a string of tokens.\n",
    "\n",
    "        >>> tokens = tuple('one one two three one two four'.split())\n",
    "        >>> unig = UnigramLM(tokens)\n",
    "        >>> samp = unig.sample(1000)\n",
    "        >>> isinstance(samp, str)\n",
    "        True\n",
    "        >>> len(samp.split()) == 1000\n",
    "        True\n",
    "        >>> s = pd.Series(samp.split()).value_counts(normalize=True).loc['one']\n",
    "        >>> np.isclose(s, 0.41, atol=0.05).all()\n",
    "        True\n",
    "        \"\"\"\n",
    "        idx = self.mdl.index\n",
    "        prob = self.mdl.to_list()\n",
    "        lst = np.random.choice(idx, M, p=prob)\n",
    "        return ' '.join(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Baseline Language Models\n",
    "\n",
    "You've now trained two baseline language models capable of generating new text from a given training text. Attempt to answer these questions for yourself before you continue.\n",
    "\n",
    "* Which model do you think is better? Why?\n",
    "* What are the ways in which both of these models are bad?\n",
    "\n",
    "If you haven't trained your models on the `shakes` corpus, uncomment and run the cells below to do so and generate new random \"sentences\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run â€“ should take less than 10 seconds\n",
    "shakes_uniform = UniformLM(shakes)\n",
    "shakes_unigram = UnigramLM(shakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unconquered misbegot Unlock incestuous benevolences youtli servingmen Sisters SENATOR eyeballs'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and run\n",
    "shakes_uniform.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forth matter doth be dream enough Faith are of power'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and run\n",
    "shakes_unigram.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='part3'></a>\n",
    "\n",
    "## Part 3: Creating an N-Gram Language Model ðŸ“š\n",
    "\n",
    "### Recap\n",
    "\n",
    "First, let's recap what we've done so far. Recall the chain rule for probability, where $w$ is a sentence made up of tokens $w_1, w_2, ..., w_n$:\n",
    "\n",
    "$$P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})$$\n",
    "\n",
    "In Questions 3 (Uniform) and 4 (Unigram), your `train` methods computed these probabilities **unconditionally**, meaning that the probability that a token appeared in a sentence did **not depend** on the other tokens around it. That is, you said $P(w_i | w_1, w_2, ..., w_{i - 1}) = P(w_i)$. In Question 3, you let $P(w_i) = \\frac{1}{\\text{# unique tokens in corpus}}$, and in Question 4, you let $P(w_i) = \\frac{\\text{# tokens in corpus equal to $w_i$}}{\\text{# tokens in corpus}}$. Cruciually, each probability was determined by looking at the corpus that the model was trained on.\n",
    "\n",
    "<br>\n",
    "\n",
    "### N-Gram Overview\n",
    "\n",
    "Now we will build an N-Gram language model, in which the probability of a token appearing in a sentence **does depend** on the tokens that come before it. \n",
    "\n",
    "The chain rule above specifies that the probability that a token occurs at in a particular position in a sentence depends on **all** previous tokens in the sentence. However, it is often the case that the likelihood that a token appears in a sentence is influenced more by **nearby** tokens. (Remember, tokens are words, punctuation, or `'\\x02'` / `'\\x03'`).\n",
    "\n",
    "The N-Gram language model relies on the assumption that only nearby tokens matter. Specifically, it assumes that the probability that a token occurs depends only on the previous $N-1$ tokens, rather than all previous tokens. That is:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "In an N-Gram language model, there is a hyperparameter that we get to choose when creating the model, $N$. For any $N$, the resulting N-Gram model looks at the previous $N-1$ tokens when computing probabilities. (Note that the unigram model you built in Question 4 is really an N-Gram model with $N=1$, since it looked at 0 previous tokens when computing probabilities.)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Example: Trigram Model\n",
    "\n",
    "When $N=3$, we have a \"trigram\" model. Such a model looks at the previous $N-1 = 2$ tokens when computing probabilities.\n",
    "\n",
    "Consider the tuple `('when', 'I', 'drink', 'Coke', 'I', 'smile')`, corresponding to the sentence `'when I drink Coke I smile'`. Under the trigram model, the probability of this sentence is computed as follows:\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I}) \\cdot P(\\text{Coke | I drink}) \\cdot P(\\text{I | drink Coke}) \\cdot P(\\text{smile | Coke I})$$\n",
    "\n",
    "The trigram model doesn't consider the beginning of the sentence when computing the probability that the sentence ends in `'smile'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### N-Grams\n",
    "\n",
    "Both when working with a training corpus and when implementing the `probability` method to compute the probabilities of other sentences, you will need to work with \"chunks\" of $N$ tokens at a time.\n",
    "\n",
    "**Definition:** The **N-Grams of a text** are a list of tuples containing sliding windows of length $N$.\n",
    "\n",
    "For instance, the trigrams in the sentence `'when I drink Coke I smile'` are:\n",
    "\n",
    "```py\n",
    "[('when', 'I', 'drink'), ('I', 'drink', 'Coke'), ('drink', 'Coke', 'I'), ('Coke', 'I', 'smile')]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Computing N-Gram Probabilities\n",
    "\n",
    "Notice in our trigram model above, we computed $P(\\text{when I drink Coke I smile})$ as being the product of several conditional probabilities. These conditional probabilities are the result of **training** our N-Gram model on a training corpus.\n",
    "\n",
    "To train an N-Gram model, we must compute a conditional probability for every $N$-token sequence in the corpus. For instance, suppose again that we are training a trigram model. Then, for every 3-token sequence $w_1, w_2, w_3$, we must compute $P(w_3 | w_1, w_2)$. To do so, we use:\n",
    "\n",
    "$$P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram sequence $w_1, w_2, w_3$ in the training corpus and $C(w_1, w_2)$ is the number of occurrences of the bigram sequence  $w_1, w_2$ in the training corpus. (Technical note: the probabilities that we compute using the ratios of counts are _estimates_ of the true conditional probabilities of N-Grams in the population of corpuses from which our corpus was drawn.)\n",
    "\n",
    "In general, for any $N$, conditional probabilities are computed by dividing the counts of N-Grams by the counts of the (N-1)-Grams they follow. \n",
    "\n",
    "**In the description of Question 5.2 we provide a detailed example of how we might compute such probabilities.**\n",
    "\n",
    "<br>\n",
    "\n",
    "### The `NGramLM` Class\n",
    "\n",
    "The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and a positive integer `N`, specifying the N in N-grams. This parameter is stored in an attribute `N`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-Grams (recall from above, an N-Gram is a **tuple** of length N). This list of N-Grams is then passed to the `train` method to train the N-Gram model.\n",
    "1. While the `train` method still creates a language model (in this case, an N-Gram model) and stores it in the `mdl` attribute, this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`, containing the N-Grams found in the text.\n",
    "    - `'n1gram'`, containing the (N-1)-Grams upon which the N-Grams in `ngram` are built.\n",
    "    - `'prob'`, containing the probabilities of each N-Gram in `ngram`.\n",
    "1. The `NGramLM` class has an attribute `prev_mdl` that stores an (N-1)-Gram language model over the same corpus (which in turn will store an (N-2)-Gram language model over the same corpus, and so on). This is necessary to compute the probability that a word occurs at the start of a text. This code is included for you in the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating N-Grams\n",
    "\n",
    "<a name='question5a'></a>\n",
    "\n",
    "Complete the implementation of the `create_ngrams` method of the `NGramLM` class, which takes in a list of tokens and returns a list of N-Grams, as a tuple. Example behavior is shown below.\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    ">>> pizza_model = NGramLM(3, tokens)\n",
    ">>> pizza_model.create_ngrams(tokens)\n",
    "[('\\x02', 'when', 'I'),\n",
    " ('when', 'I', 'eat'),\n",
    " ('I', 'eat', 'pizza'),\n",
    " ('eat', 'pizza', ','),\n",
    " ('pizza', ',', 'I'),\n",
    " (',', 'I', 'smile'),\n",
    " ('I', 'smile', ','),\n",
    " ('smile', ',', 'but'),\n",
    " (',', 'but', 'when'),\n",
    " ('but', 'when', 'I'),\n",
    " ('when', 'I', 'drink'),\n",
    " ('I', 'drink', 'Coke'),\n",
    " ('drink', 'Coke', ','),\n",
    " ('Coke', ',', 'my'),\n",
    " (',', 'my', 'stomach'),\n",
    " ('my', 'stomach', 'hurts'),\n",
    " ('stomach', 'hurts', '\\x03')]\n",
    "```\n",
    "\n",
    "Make sure you understand the above behavior before implementing `create_ngrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('when', 'I', 'eat')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(tokens[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\x02', 'when', 'I'),\n",
       " ('when', 'I', 'eat'),\n",
       " ('I', 'eat', 'pizza'),\n",
       " ('eat', 'pizza', ','),\n",
       " ('pizza', ',', 'I'),\n",
       " (',', 'I', 'smile'),\n",
       " ('I', 'smile', ','),\n",
       " ('smile', ',', 'but'),\n",
       " (',', 'but', 'when'),\n",
       " ('but', 'when', 'I'),\n",
       " ('when', 'I', 'drink'),\n",
       " ('I', 'drink', 'Coke'),\n",
       " ('drink', 'Coke', ','),\n",
       " ('Coke', ',', 'my'),\n",
       " (',', 'my', 'stomach'),\n",
       " ('my', 'stomach', 'hurts'),\n",
       " ('stomach', 'hurts', '\\x03')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    "tokens = tokenize(corpus)\n",
    "result = []\n",
    "N = 3\n",
    "for i in range(len(tokens)-N+1):\n",
    "    result.append(tuple(tokens[i:i+N]))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the N-Gram LM\n",
    "\n",
    "<a name='question5b'></a>\n",
    "\n",
    "Now, compute the probabilities that define N-Gram language model itself. Recall that the N-Gram LM consists of probabilities of the form\n",
    "\n",
    "$$P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "which we estimate by  \n",
    "\n",
    "$$\\frac{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1}, w_n)}{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1})}$$\n",
    "\n",
    "for every N-Gram that occurs in the corpus. To illustrate, consider again the following example corpus:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    ">>> pizza_model = NGrams(3, tokens)\n",
    "```\n",
    "\n",
    "Here, `pizza_model.train` must compute $P(\\text{I | \\x02 when})$, $P(\\text{eat | when I})$, $P(\\text{pizza | I eat})$, and so on, until $P(\\text{\\x03 | stomach hurts})$.\n",
    "\n",
    "To compute $P(\\text{eat | when I})$, we must find the number of occurrences of `'when I eat'` in the training corpus, and divide it by the number of occurrences of `'when I'` in the training corpus. `'when I eat'` occurred exactly once in the training corpus, while `'when I'` occurred twice, so,\n",
    "\n",
    "$$P(\\text{eat | when I}) = \\frac{C(\\text{when I eat})}{C(\\text{when I})} = \\frac{1}{2}$$\n",
    "\n",
    "To store the conditional probabilities of all N-Grams, we will use a DataFrame with three columns, like so:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ngram</th>\n",
    "      <th>n1gram</th>\n",
    "      <th>prob</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>(when, I, drink)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>(when, I, eat)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>(,, but, when)</td>\n",
    "      <td>(,, but)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>(,, I, smile)</td>\n",
    "      <td>(,, I)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>(I, smile, ,)</td>\n",
    "      <td>(I, smile)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>(,, my, stomach)</td>\n",
    "      <td>(,, my)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>(but, when, I)</td>\n",
    "      <td>(but, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>(\u0002, when, I)</td>\n",
    "      <td>(\u0002, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>(stomach, hurts, \u0003)</td>\n",
    "      <td>(stomach, hurts)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>(Coke, ,, my)</td>\n",
    "      <td>(Coke, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>(eat, pizza, ,)</td>\n",
    "      <td>(eat, pizza)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>(I, drink, Coke)</td>\n",
    "      <td>(I, drink)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>(my, stomach, hurts)</td>\n",
    "      <td>(my, stomach)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>(pizza, ,, I)</td>\n",
    "      <td>(pizza, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>(I, eat, pizza)</td>\n",
    "      <td>(I, eat)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>(drink, Coke, ,)</td>\n",
    "      <td>(drink, Coke)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>(smile, ,, but)</td>\n",
    "      <td>(smile, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "N = 2\n",
    "for i in range(len(shakes)-N+1):\n",
    "    ngrams.append(tuple(shakes[i:i+N]))\n",
    "df = pd.DataFrame({'ngram': ngrams})\n",
    "df = df.assign(n1gram = df['ngram'].apply(lambda x: x[:(N-1)]))\n",
    "        \n",
    "        # N-Gram counts C(w_1, ..., w_n)\n",
    "result_df = pd.DataFrame(df['ngram'].value_counts()).reset_index()\n",
    "result_df.columns = ['ngram', 'ngram_count']\n",
    "result_df = result_df.assign(n1gram_idx = result_df['ngram'].apply(lambda x: x[:(N-1)])).set_index('n1gram_idx')\n",
    "        \n",
    "        # (N-1)-Gram counts C(w_1, ..., w_(n-1))\n",
    "n1gram_count = pd.DataFrame(df['n1gram'].value_counts())\n",
    "\n",
    "result_df = result_df.merge(n1gram_count, left_index=True, right_index=True).reset_index()\n",
    "result_df.columns = ['n1gram','ngram', 'ngram_count', 'n1gram_count']\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'ngram': result})\n",
    "df = df.assign(n1gram = df['ngram'].apply(lambda x: x[:(N-1)]))\n",
    "df_count = pd.DataFrame(index=df.index)\n",
    "        \n",
    "# N-Gram counts C(w_1, ..., w_n)\n",
    "df_count['ngram_count'] = df.groupby('ngram', as_index=False)['ngram'].transform(lambda x: x.count())\n",
    "        \n",
    "# (N-1)-Gram counts C(w_1, ..., w_(n-1))\n",
    "df_count['n1gram_count'] = df.groupby('n1gram', as_index=False)['n1gram'].transform(lambda x: x.count())\n",
    "\n",
    "# Create the conditional probabilities\n",
    "df = df.assign(prob = df_count['ngram_count']/df_count['n1gram_count'])\n",
    "# Put it all together\n",
    "choice = df[df['n1gram'] == ('when', 'I')]['ngram'].apply(lambda x: x[-1]).tolist()\n",
    "prob = df[df['n1gram'] == ('when', 'I')]['prob'].values.tolist()\n",
    "\n",
    "np.random.choice(choice, 1, p=prob)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Probabilities using the N-Gram Model\n",
    "\n",
    "<a name='question5c'></a>\n",
    "\n",
    "After we've trained our N-Gram model â€“ that is, after we've computed a DataFrame associating each N-Gram with a conditional probability â€“ we need to compute probabilities for new sentences.\n",
    "\n",
    "To illustrate how this may work, let's look at an example input tuple to `probability`. Assume our model is `pizza_model` from Question 5.2's description; we will not repeat the probability table here.\n",
    "\n",
    "Suppose our input tuple is `('when', 'I', 'eat', 'pizza', ',', 'I', 'smile')`, corresponding to the sentence `'when I eat pizza, I smile'` (remember again that the tuples provided to `probability` don't need to include `'\\x02'` or `'\\x03'`). Then,\n",
    "\n",
    "$$\n",
    "\\begin{align*} &P(\\text{when I eat pizza, I smile}) \\\\ &= P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{eat | when I}) \\cdot P(\\text{pizza | I eat}) \\cdot P(\\text{, | eat pizza}) \\cdot P(\\text{I | pizza,})\\cdot P(\\text{smile | , I}) \\\\ &= \\frac{2}{19} \\cdot 1 \\cdot \\frac{1}{2} \\cdot 1 \\cdot 1 \\cdot 1 \\cdot 1 \\\\ &= \\frac{1}{19} \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "- To find the latter five probabilities â€“ $P(\\text{eat | when I}) , P(\\text{pizza | I eat}) , P(\\text{, | eat pizza}) , P(\\text{I | pizza,}),$ and $P(\\text{smile | , I})$, we can use the `mdl` DataFrame that the `train` method computes.\n",
    "- To find $P(\\text{I | when})$, we can't just look at the `mdl` DataFrame, because `('when', 'I')` is not a trigram, it is a bigram. Instead, we look at our model's `prev_mdl` attribute, which itself is another instance of `NGramLM`, corresponding to a bigram model over the same corpus. There, we can find the probability $P(\\text{I | when})$.\n",
    "- To find $P(\\text{when})$, we can't just look at the `mdl` DataFrame, because `'when'` is not a trigram. It is not a bigram either. Instead, we need to look at `prev_mdl`'s `prev_mdl`, which is a `UnigramLM`, to find $P(\\text{when})$.\n",
    "\n",
    "Note that if the input tuple contains an N-Gram that was never seen in the training corpus, the returned probability is 0. Convince yourself why `pizza_model.probability(('when', 'I', 'drink', 'Coke', ',', 'I', 'smile'))` is 0 before proceeding.\n",
    "\n",
    "After you've understood the above example output, complete the implementation of the `probability` method in `NGramLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_test = tuple('\\x02 one two one three one two \\x03'.split())\n",
    "bigrams = NGramLM(2, tokens_test)\n",
    "p = bigrams.probability('two one three'.split())\n",
    "np.isclose(p, (1/4) * (1/2) * (1/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4 â€“ Sampling from the N-Gram Model\n",
    "\n",
    "<a name='question5d'></a>\n",
    "\n",
    "The last method you implemented in the `UniformLM` and `UnigramLM` classes was `sample`, which gave you a way of generating new sentences. \n",
    "\n",
    "Now, you will implement the `sample` method in the `NGramLM` class. It should take in a positive integer `M` and generate a string of M tokens using the trained language model. It should begin with a starting token `'\\x02'`, then generate subsequent tokens from the probabilities in `self.mdl` and continue picking words conditional on the previous choice. \n",
    "\n",
    "Let's illustrate how sampling works using a small concrete example. Suppose our corpus and **trigram** model are defined below:\n",
    "\n",
    "```py\n",
    ">>> short_corpus = 'zebras eat green peas \\n\\n cows eat green grass \\n\\n zebras eat green peppers'\n",
    ">>> short_tokens = tokenize(short_corpus)\n",
    ">>> short_tokens\n",
    "['\\x02', 'zebras', 'eat', 'green', 'peas', '\\x03', '\\x02', 'cows', 'eat', 'green', 'grass', '\\x03', '\\x02', 'zebras', 'eat', 'green', 'peppers', '\\x03']\n",
    ">>> grass_model = NGramLM(3, short_tokens)\n",
    "```\n",
    "\n",
    "Suppose we are told to execute `grass_model.sample(5)`. Here's how we'd proceed:\n",
    "\n",
    "0. The first character in the output is `'\\x02'`, as specified above. **We won't count `'\\x02'` in the length of our output string**, so we still need to find 5 more tokens.\n",
    "1. The next character needs to be either `'zebras'` or `'cows'`, since `('\\x02', 'zebras')` and `('\\x02', 'cows')` are the only **bigrams** in `short_tokens` that start with an `'\\x02'`. $P(\\text{zebras | \\x02})$ is $\\frac{2}{3}$ and $P(\\text{cows | \\x02})$ is $\\frac{1}{3}$, so we select either `'zebras'` or `'cows'` for our next token according to these probabilities. For the sake of example, suppose we select `'cows'`. 4 more tokens to go.\n",
    "2. Now, we must look for **trigrams** that start with the bigram `('\\x02', 'cows')`. There is just one, `('\\x02', 'cows', 'eat')`, so our next token must be `'eat'`. 3 more tokens to go.\n",
    "3. Now, we must look for **trigrams** that start with the bigram `('cows', 'eat')`. Again, there is just one, `('cows', 'eat', 'green')`, so our next token must be `'green'`. 2 more tokens to go.\n",
    "4. Now, we must look for **trigrams** that start with the bigram `('eat', 'green')`. There are three options â€“ `('eat', 'green', 'peas')`, `('eat', 'green', 'grass')`, and `('eat', 'green', 'peppers')`. Since $P(\\text{peas | eat green}) = P(\\text{grass | eat green}) = P(\\text{peppers | eat green}) = \\frac{1}{3}$, we pick either `'peas'`, `'grass'`, or `'peppers'` uniformly at random. For the sake of example, suppose we select `'peppers'`. 1 more token to go.\n",
    "5. We must end the output string now with `'\\x03'`, putting us at `'\\x02'` plus 5 tokens, which is the number of tokens we were told to sample. Note that `'\\x03'` **does** count towards the number of tokens we were asked to sample.\n",
    "\n",
    "Our result is `'\\x02 cows eat green peppers \\x03'`. **Note that in our training corpus we never encountered an instance of cows ðŸ„ eating green peppers ðŸ«‘, but we were able to generate a coherent sentence in which they did â€“ pretty cool!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_corpus = 'zebras eat green peas \\n\\n cows eat green grass \\n\\n zebras eat green peppers'\n",
    "short_tokens = tokenize(short_corpus)\n",
    "short_tokens\n",
    "grass_model = NGramLM(3, short_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\x02',)\n",
      "('\\x02', 'one')\n",
      "('one', 'two')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\x02', 'one', 'two', 'three']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 57\n"
     ]
    }
   ],
   "source": [
    "tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "bigrams = NGramLM(3, tokens)\n",
    "samp = bigrams.sample(3)\n",
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n1gram</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Humpty, Dumpty)</td>\n",
       "      <td>(Humpty,)</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(Humpty, together)</td>\n",
       "      <td>(Humpty,)</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ngram     n1gram      prob\n",
       "1     (Humpty, Dumpty)  (Humpty,)  0.666667\n",
       "34  (Humpty, together)  (Humpty,)  0.333333"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_5b1[out_5b1['n1gram'] == ('Humpty',)].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NGramLM(object):\n",
    "    \n",
    "    def __init__(self, N, tokens):\n",
    "        \"\"\"\n",
    "        Initializes a N-gram languange model using a\n",
    "        list of tokens. It trains the language model\n",
    "        using `train` and saves it to an attribute\n",
    "        self.mdl.\n",
    "        \"\"\"\n",
    "        # You don't need to edit the constructor,\n",
    "        # but you should understand how it works!\n",
    "        \n",
    "        self.N = N\n",
    "\n",
    "        ngrams = self.create_ngrams(tokens)\n",
    "\n",
    "        self.ngrams = ngrams\n",
    "        self.mdl = self.train(ngrams)\n",
    "\n",
    "        if N < 2:\n",
    "            raise Exception('N must be greater than 1')\n",
    "        elif N == 2:\n",
    "            self.prev_mdl = UnigramLM(tokens)\n",
    "        else:\n",
    "            self.prev_mdl = NGramLM(N-1, tokens)\n",
    "\n",
    "    def create_ngrams(self, tokens):\n",
    "        \"\"\"\n",
    "        create_ngrams takes in a list of tokens and returns a list of N-grams. \n",
    "        The START/STOP tokens in the N-grams should be handled as \n",
    "        explained in the notebook.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "        >>> bigrams = NGramLM(2, [])\n",
    "        >>> out = bigrams.create_ngrams(tokens)\n",
    "        >>> isinstance(out[0], tuple)\n",
    "        True\n",
    "        >>> out[0]\n",
    "        ('\\\\x02', 'one')\n",
    "        >>> out[2]\n",
    "        ('two', 'three')\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in range(len(tokens)-self.N+1):\n",
    "            result.append(tuple(tokens[i:i+self.N]))\n",
    "        return result\n",
    "        \n",
    "    def train(self, ngrams):\n",
    "        \"\"\"\n",
    "        Trains a n-gram language model given a list of tokens.\n",
    "        The output is a dataframe with three columns (ngram, n1gram, prob).\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "        >>> bigrams = NGramLM(2, tokens)\n",
    "        >>> set(bigrams.mdl.columns) == set('ngram n1gram prob'.split())\n",
    "        True\n",
    "        >>> bigrams.mdl.shape == (6, 3)\n",
    "        True\n",
    "        >>> bigrams.mdl['prob'].min() == 0.5\n",
    "        True\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({'ngram': ngrams})\n",
    "        df = df.assign(n1gram = df['ngram'].apply(lambda x: x[:(self.N-1)]))\n",
    "        \n",
    "        # N-Gram counts C(w_1, ..., w_n)\n",
    "        result_df = pd.DataFrame(df['ngram'].value_counts()).reset_index()\n",
    "        result_df.columns = ['ngram', 'ngram_count']\n",
    "        result_df = result_df.assign(n1gram_idx = result_df['ngram'].apply(lambda x: x[:(self.N-1)])).set_index('n1gram_idx')\n",
    "        \n",
    "        # (N-1)-Gram counts C(w_1, ..., w_(n-1))\n",
    "        n1gram_count = pd.DataFrame(df['n1gram'].value_counts())\n",
    "        result_df = result_df.merge(n1gram_count, left_index=True, right_index=True).reset_index()\n",
    "        result_df.columns = ['n1gram', 'ngram', 'ngram_count', 'n1gram_count']\n",
    "        \n",
    "        # Create the conditional probabilities\n",
    "        result_df['prob'] = result_df['ngram_count']/result_df['n1gram_count']\n",
    "        result_df = result_df.drop(columns=['ngram_count', 'n1gram_count'])\n",
    "\n",
    "        # Put it all together\n",
    "        result_df = result_df.reindex(columns=['ngram', 'n1gram', 'prob'])\n",
    "        return result_df\n",
    "    \n",
    "    def probability(self, words):\n",
    "        \"\"\"\n",
    "        probability gives the probabiliy a sequence of words\n",
    "        appears under the language model.\n",
    "        :param: words: a tuple of tokens\n",
    "        :returns: the probability `words` appears under the language\n",
    "        model.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('\\x02 one two one three one two \\x03'.split())\n",
    "        >>> bigrams = NGramLM(2, tokens)\n",
    "        >>> p = bigrams.probability('two one three'.split())\n",
    "        >>> np.isclose(p, (1/4) * (1/2) * (1/3))\n",
    "        True\n",
    "        >>> bigrams.probability('one two five'.split()) == 0\n",
    "        True\n",
    "        \"\"\"\n",
    "        test = self.create_ngrams(words)\n",
    "        p = 1\n",
    "        \n",
    "        for token in test:\n",
    "            if token in self.ngrams:\n",
    "                p = p*self.mdl.loc[self.mdl['ngram'] == token, 'prob'].iloc[0]\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        prev = self.prev_mdl\n",
    "        for i in range(self.N-1, 0, -1):\n",
    "            if i+1 > 2:\n",
    "                token_temp = words[:i]\n",
    "                if token_temp in prev.ngrams:\n",
    "                    p = p*prev.mdl.loc[prev.mdl['ngram'] == token_temp, 'prob'].iloc[0]\n",
    "                else:\n",
    "                    return 0\n",
    "                prev = prev.prev_mdl\n",
    "            else:\n",
    "                p = p*prev.probability(words[:i])\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def sample(self, M):\n",
    "        \"\"\"\n",
    "        sample selects tokens from the language model of length M, returning\n",
    "        a string of tokens.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "        >>> bigrams = NGramLM(2, tokens)\n",
    "        >>> samp = bigrams.sample(3)\n",
    "        >>> len(samp.split()) == 4  # don't count the initial START token.\n",
    "        True\n",
    "        >>> samp[:2] == '\\\\x02 '\n",
    "        True\n",
    "        >>> set(samp.split()) <= {'\\\\x02', '\\\\x03', 'one', 'two', 'three', 'four'}\n",
    "        True\n",
    "        \"\"\"\n",
    "        result = ['\\x02']\n",
    "\n",
    "        # Use a helper function to generate sample tokens of length `length`\n",
    "        #base case\n",
    "        def gen_sample(M, result, model=self):\n",
    "            if len(result) < M+1:\n",
    "                if len(result) >= model.N - 1:\n",
    "                    #say now we r at bigram\n",
    "                    n1gram = tuple(result[-(model.N-1):])\n",
    "                    selected_df = model.mdl[model.mdl['n1gram'] == n1gram].drop_duplicates()\n",
    "                    choice = selected_df['ngram'].apply(lambda x: x[-1]).tolist()\n",
    "                    prob = selected_df['prob'].values.tolist()\n",
    "                    if(len(choice) == 0):\n",
    "                        result.append('\\x03')\n",
    "                    else:\n",
    "                        to_add = np.random.choice(choice, 1, p=prob)[0]\n",
    "                        result.append(to_add)\n",
    "                    return gen_sample(M, result)\n",
    "                else:\n",
    "                    return gen_sample(M, result, model=model.prev_mdl)\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        \n",
    "        # Transform the tokens to strings\n",
    "        result = gen_sample(M, result)\n",
    "        result[-1] = '\\x03'\n",
    "        return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "# note â€“ these tests are different than the doctests; you should run them both \n",
    "tokens = \"\\x02 Humpty Dumpty sat on a wall , Humpty Dumpty had a great fall . \\x03 \\x02 All the king ' s horses and all the king ' s men couldn ' t put Humpty together again . \\x03\".split()\n",
    "tokens = tuple(tokens)\n",
    "ngram = NGramLM(2, tokens)\n",
    "out_5a1 = ngram.create_ngrams(tokens)\n",
    "out_5b1 = ngram.mdl\n",
    "out_5c1 = ngram\n",
    "out_5d1 = ngram.sample(500) \n",
    "out_5d1\n",
    "np.isclose(pd.Series(out_5d1.split()).value_counts(normalize=True).iloc[0], 0.07, atol=0.03)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> beowulf[:20] == '\\n\\n\\n\\n\\nProduced by Dav'\nTrue",
         "failure_message": "make sure front matter not in string",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> license = 're-use it under the terms of the Project Gutenberg License'\n>>> license not in beowulf\nTrue",
         "failure_message": "make sure front matter not in string",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> story = 'king of the Danes, or Scyldings'\n>>> story in beowulf\nTrue",
         "failure_message": "text from book check",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> story = 'Said he was kindest of kings'\n>>> story in beowulf\nTrue",
         "failure_message": "text from book check",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> end = 'This file should be named 16328-8.txt'\n>>> end not in beowulf\nTrue",
         "failure_message": "end matter check",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 1000000 <= len(shakes) <= 1500000\nTrue",
         "failure_message": "approx correct number of tokens",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> elapsed <= 10\nTrue",
         "failure_message": "shakespeare fast enough",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> 1000000 <= len(shakes) <= 1500000\nTrue",
         "failure_message": "approx correct number of tokens",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> shakes[:3] == ['\\x02', 'The', 'Complete']\nTrue",
         "failure_message": "check beginning",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> shakes[-3:] == ['William', 'Shakespeare', '\\x03']\nTrue",
         "failure_message": "check ending",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> shakes[41] == 'IT'\nTrue",
         "failure_message": "Check specific token",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 'youth' in shakes[490:510]\nTrue",
         "failure_message": "approx correct number of tokens",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (unif.mdl == 0.25).all()\nTrue",
         "failure_message": "only one probability",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> unif.mdl.shape[0] == 4\nTrue",
         "failure_message": "number of tokens in mdl",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(unif.mdl, pd.Series)\nTrue",
         "failure_message": "mdl correct type",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> set(unif.mdl.index) == set('one two three four'.split())\nTrue",
         "failure_message": "correct indices for mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unif.probability(('five',)) == 0\nTrue",
         "failure_message": "five not a token",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unif.probability(('one', 'two')) == 0.0625\nTrue",
         "failure_message": "probability of given sequence appearing",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> np.isclose(unif.probability(('one', 'two', 'three', 'two', 'one', 'four', 'one', 'two')), 0.25 ** 8, atol=1e-6)\nTrue",
         "failure_message": "probability of given sequence appearing",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(unif.sample(1000), str)\nTrue",
         "failure_message": "sample is string",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> len(unif.sample(1000).split()) == 1000\nTrue",
         "failure_message": "length of sample",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> s = pd.Series(unif.sample(1000).split()).value_counts(normalize=True)\n>>> np.isclose(s, 0.25, atol=0.05).all()\nTrue",
         "failure_message": "prop of words in sample close to 0.25",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(unigram.mdl, pd.Series)\nTrue",
         "failure_message": "mdl correct type",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> set(unigram.mdl.index) == set('one two three four'.split())\nTrue",
         "failure_message": "correct indices for mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> np.isclose(unigram.mdl.loc['one'], 3/7, atol=0.005)\nTrue",
         "failure_message": "mdl: one",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unigram.probability(('five',)) == 0\nTrue",
         "failure_message": "five not a token",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> p = unigram.probability(('one', 'two')) \n>>> np.isclose(p, 0.12244897959, atol=0.0001)\nTrue",
         "failure_message": "probability of given sequence appearing",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> p = unigram.probability(('one', 'two', 'one', 'one', 'four'))\n>>> p_out = (unigram.mdl['one'] ** 3) * unigram.mdl['two'] * unigram.mdl['four']\n>>> np.isclose(p, p_out, atol=0.0001)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> len(unigram.sample(1000).split()) == 1000\nTrue",
         "failure_message": "sample length",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> isinstance(unigram.sample(1000), str)\nTrue",
         "failure_message": "sample is string",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> s = pd.Series(unigram.sample(1000).split()).value_counts(normalize=True).loc['one']\n>>> np.isclose(s, 0.41, atol=0.05).all()\nTrue",
         "failure_message": "prop of words in sample close to observed prop, 0.41",
         "hidden": false,
         "locked": false,
         "points": 0.25
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 36 <= len(out_5a1) <= 40\nTrue",
         "failure_message": "test length",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> all([len(x) == 2 for x in out_5a1])\nTrue",
         "failure_message": "ngram lengths",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out_5a1[0] == ('\\x02', 'Humpty')\nTrue",
         "failure_message": "padded with START",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> ('Humpty', 'Dumpty') in out_5a1\nTrue",
         "failure_message": "Humpty Dumpty in bigrams",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> ('great', 'fall') in out_5a1\nTrue",
         "failure_message": "specific bigram in bigrams",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
